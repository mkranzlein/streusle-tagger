{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook starts in notebooks folder. Change working directory back to streusle-tagger\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# Add parent of streusle-tagger to path (streusle should be in this folder)\n",
    "sys.path.append(\"../../..\")\n",
    "\n",
    "# External imports\n",
    "import allennlp.nn.util as util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.util import import_submodules\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.dataset import Batch\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.nn.util import logsumexp\n",
    "from allennlp.training.optimizers import Optimizer\n",
    "from allennlp.training.util import datasets_from_params\n",
    "\n",
    "import_submodules(\"streusle_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params.from_file(\"training_config/streusle_bert_large_cased/streusle_bert_large_cased_no_constraints.jsonnet\")\n",
    "\n",
    "# Old model\n",
    "# archive = load_archive(\"models/streusle_bert_large_cased_no_constraints/model.tar.gz\")\n",
    "\n",
    "archive = load_archive(\"models/new_no_constraints/model.tar.gz\")\n",
    "model = archive.model\n",
    "\n",
    "index_to_label = model.vocab.get_index_to_token_vocabulary(model._label_namespace)\n",
    "label_to_index = dict(zip(index_to_label.values(), index_to_label.keys()))\n",
    "\n",
    "labels_df = pd.DataFrame(label_to_index, columns=[\"Label\", \"Index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets_from_params(deepcopy(params))\n",
    "dataset_reader_params = deepcopy(params).pop(\"dataset_reader\")\n",
    "dataset_reader = DatasetReader.from_params(dataset_reader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimsum_test_path = \"data/dimsum16/dimsum16_test_updated_labeled.json\"\n",
    "dimsum_test_reformatted = \"data/dimsum16/dimsum16_test_updated_labeled_reformatted.json\"\n",
    "\n",
    "with open(dimsum_test_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "newlines = []\n",
    "\n",
    "newlines.append(\"[\\n\")\n",
    "for i, line in enumerate(lines):\n",
    "    if i != len(lines) - 1:\n",
    "        newlines.append(line[:-1] + \",\\n\")\n",
    "    else:\n",
    "        newlines.append(line[:-1] + \"\\n\")\n",
    "newlines.append(\"]\")\n",
    "        \n",
    "with open(dimsum_test_reformatted, \"w\") as f:\n",
    "    f.writelines(newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(file_path):\n",
    "    with open(file_path, 'r') as tagging_file:\n",
    "        tagging_data = json.load(tagging_file)\n",
    "        for i, x in enumerate(tagging_data):\n",
    "            if i % 200 == 0:\n",
    "                print(i)\n",
    "            tokens = [_ for _ in x[\"tokens\"]]\n",
    "            # Get their associated upos\n",
    "            upos_tags = [_ for _ in x[\"upos_tags\"]]\n",
    "\n",
    "            # Get their associated lemma\n",
    "            lemmas = [_ for _ in x[\"lemmas\"]]\n",
    "            \n",
    "            # Don't need ground labels for confidence scores\n",
    "            \n",
    "            yield dataset_reader.text_to_instance(tokens, upos_tags, lemmas)\n",
    "            \n",
    "dimsum_test = list(read(dimsum_test_reformatted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denominator(crf, logits):\n",
    "    \"\"\"Sum of all paths through the CRF.\"\"\"\n",
    "    if len(logits.size()) > 1:\n",
    "        sequence_length, num_tags = logits.size()\n",
    "        alpha = crf.start_transitions + logits[0]\n",
    "    else:\n",
    "        sequence_length = 1\n",
    "        num_tags = logits.size()[0]\n",
    "        alpha = crf.start_transitions + logits\n",
    "        \n",
    "    forward_trellis = []\n",
    "    forward_trellis.append(alpha)\n",
    "    \n",
    "    for i in range(1, sequence_length):\n",
    "        forward_trellis.append(forward_trellis[i - 1] + logsumexp(logits[i].view(1, num_tags) + crf.transitions))\n",
    "\n",
    "    denom = util.logsumexp(forward_trellis[sequence_length - 1] + crf.end_transitions)\n",
    "    \n",
    "    backward_trellis = []\n",
    "\n",
    "    if sequence_length > 1:\n",
    "        backward_trellis.append(logsumexp(logits[sequence_length - 1].view(1, num_tags) + crf.transitions))\n",
    "\n",
    "    reverse_indexes = list(range(1, sequence_length - 1))\n",
    "    reverse_indexes.reverse()\n",
    "    for i in reverse_indexes:\n",
    "        backward_trellis.append(backward_trellis[sequence_length - i - 2] + logsumexp(logits[i].view(1, num_tags) + crf.transitions))\n",
    "        \n",
    "    # This never gets used; it's just for more intuitive indexing in numerator calculation\n",
    "    backward_trellis.append([\"dummy placeholder\"])\n",
    "    backward_trellis.reverse()\n",
    "    return forward_trellis, backward_trellis, denom\n",
    "\n",
    "def numerators(crf, logits, forward_trellis, backward_trellis, word_num):\n",
    "    \"\"\"Sum of all paths through each tag at position word_num.\"\"\"\n",
    "    if len(logits.size()) > 1:\n",
    "        sequence_length, num_tags = logits.size()\n",
    "    else:\n",
    "        sequence_length = 1\n",
    "        num_tags = logits.size()[0]\n",
    "    \n",
    "    if sequence_length == 1:\n",
    "        transition_mask = torch.zeros((num_tags, num_tags))\n",
    "        for i in range(num_tags):\n",
    "            transition_mask[i, i] = 1\n",
    "        alpha = util.replace_masked_values(model.crf.start_transitions.repeat(num_tags, 1) + logits.repeat(num_tags, 1), transition_mask, -1e32)\n",
    "        return logsumexp(alpha + model.crf.end_transitions.repeat(num_tags, 1))   \n",
    "    \n",
    "    elif word_num == 0:\n",
    "        transition_mask = torch.zeros((num_tags, num_tags))\n",
    "        for i in range(num_tags):\n",
    "            transition_mask[i, i] = 1\n",
    "        alpha = util.replace_masked_values(model.crf.start_transitions.repeat(num_tags, 1) + logits[0].repeat(num_tags, 1), transition_mask, -1e32)\n",
    "        return logsumexp(alpha + backward_trellis[1].repeat(num_tags, 1) + model.crf.end_transitions.repeat(num_tags, 1))\n",
    "    \n",
    "    else:\n",
    "        emit_mask = torch.zeros((num_tags, num_tags))\n",
    "        for i in range(num_tags):\n",
    "            emit_mask[i, i] = 1\n",
    "        tiled_logits = logits[word_num].repeat(num_tags, 1)\n",
    "        emit_scores = util.replace_masked_values(tiled_logits, emit_mask, -1e32)\n",
    "\n",
    "        transition_mask = torch.zeros(num_tags, num_tags, num_tags)\n",
    "        for i in range(num_tags):\n",
    "            transition_mask[i, :, i] = 1\n",
    "\n",
    "        transition_scores = util.replace_masked_values(crf.transitions.repeat(num_tags, 1, 1), transition_mask, -1e32)\n",
    "\n",
    "        if word_num == sequence_length - 1:\n",
    "            return logsumexp(forward_trellis[word_num - 1].repeat(num_tags, 1) + logsumexp(emit_scores.view(num_tags, 1, num_tags) + transition_scores) + model.crf.end_transitions.repeat(num_tags, 1))\n",
    "        else:\n",
    "            return logsumexp(forward_trellis[word_num - 1].repeat(num_tags, 1) + logsumexp(emit_scores.view(num_tags, 1, num_tags) + transition_scores) + backward_trellis[word_num + 1].repeat(num_tags, 1) + model.crf.end_transitions.repeat(num_tags, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_confidence(crf, sequence_logits):\n",
    "    \"\"\"Calculates matrix of confidence scores with num_words rows and num_tags columns.\"\"\"\n",
    "    confidence_matrix = []\n",
    "    num_tags = crf.num_tags\n",
    "    if len(sequence_logits.size()) == 1:\n",
    "        num_words = 1\n",
    "    else:\n",
    "        num_words = sequence_logits.size()[0]\n",
    "    \n",
    "    forward_trellis, backward_trellis, denom = denominator(model.crf, sequence_logits)\n",
    "    for word_num in range(num_words):\n",
    "        nums = numerators(crf, sequence_logits, forward_trellis, backward_trellis, word_num)\n",
    "        new_row = [math.exp(num - denom) for num in nums]\n",
    "        confidence_matrix.append(new_row)\n",
    "        \n",
    "    return confidence_matrix\n",
    "        \n",
    "def dataset_confidence_no_labels(dataset, dataset_name):\n",
    "    \"\"\"Creates one CSV file per sentence, containing metadata and confidence scores for all tag-token pairs.\"\"\"\n",
    "    \n",
    "    save_path = f\"calibration/confidence_scores/{dataset_name}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    for i, instance in enumerate(dataset):\n",
    "        instance_batch = Batch([instance])\n",
    "        instance_batch.index_instances(model.vocab)\n",
    "\n",
    "        # Confidence scores\n",
    "        print(f\"Calculating confidence scores for instance {i}...\")\n",
    "        tokens = instance_batch.as_tensor_dict()[\"tokens\"]\n",
    "        embedded_tokens = model.text_field_embedder(tokens)\n",
    "        logits = model.tag_projection_layer(embedded_tokens).squeeze()\n",
    "        confidence_matrix = sentence_confidence(model.crf, logits)\n",
    "\n",
    "        # Metadata\n",
    "        tokens_list = np.array([[str(t) for t in instance.get(\"tokens\").tokens]]).transpose()\n",
    "        predicted_tags_indexes = (model.forward(**instance_batch.as_tensor_dict())[\"tags\"])[0]\n",
    "        predicted_tags = np.array([[index_to_label[i] for i in predicted_tags_indexes]]).transpose()\n",
    "        predicted_tags_indexes = np.array([predicted_tags_indexes]).transpose()\n",
    "        metadata = np.concatenate((tokens_list, predicted_tags, predicted_tags_indexes), axis=1)\n",
    "\n",
    "        # Combine metadata and confidence scores\n",
    "        data = np.concatenate((metadata, confidence_matrix), axis=1)\n",
    "        \n",
    "        # Write to file\n",
    "        columns = [\"Tokens\", \"Predicted Tags\", \"Predicted Tag Indexes\"] + [i for i in range(model.crf.num_tags)]\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        df.to_csv(f\"{save_path}/{i:04d}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_confidence_no_labels(dimsum_test, \"dimsum_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allen",
   "language": "python",
   "name": "allen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
