{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Calibration Error with Adaptive Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "confidence_path = \"../confidence_results/validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Invalid Confidence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{confidence_path}/0.csv\")\n",
    "\n",
    "# 6 metadata columns; rest are tags\n",
    "num_tags = len(df.columns) - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_invalid_scores():\n",
    "    \"\"\"Prints any confidence scores greater than 1. There shouldn't be any.\"\"\"\n",
    "    for filename in os.listdir(confidence_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            df = pd.read_csv(f\"{confidence_path}/{filename}\")\n",
    "            for t in range(0, num_tags):\n",
    "                for j, x in enumerate(df[f\"{t}\"]):\n",
    "                    if x > 1:\n",
    "                        print(f\"Sentence {i}, tag {t}, word {j}, score {x}, \\t {df.iloc[j][3]} \\t {df.iloc[j][5]} {t}\")\n",
    "\n",
    "print_invalid_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the CSVs for all sentences\n",
    "dfs = []\n",
    "for filename in os.listdir(confidence_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        df = pd.read_csv(f\"{confidence_path}/{filename}\")\n",
    "        dfs.append(df)\n",
    "df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_label_calibration_error(confidence_scores, labels, num_bins):\n",
    "    \"\"\"Calculates calibration error using adaptive binning.\"\"\"\n",
    "    \n",
    "    bins = []\n",
    "    for i in range(num_bins):\n",
    "        bins.append([])\n",
    "    b = 0\n",
    "    # TODO: handle last bin by merging last bin if it's not items_per_bin-sized\n",
    "    items_per_bin = len(confidence_scores) / num_bins\n",
    "    sorted_data = sorted(zip(confidence_scores, labels), key=lambda x: x[0])\n",
    "    for (score, label) in sorted_data:\n",
    "        if len(bins[b]) < items_per_bin:\n",
    "            bins[b].append((score, label))\n",
    "        else:\n",
    "            b += 1\n",
    "            bins[b].append((score, label))\n",
    "    \n",
    "    bin_square_errors = []\n",
    "    \n",
    "    for i, b in enumerate(bins):\n",
    "        average_score = sum([x for (x, y) in b]) / len(b)\n",
    "        average_label = sum([y for (x, y) in b]) / len(b)\n",
    "        square_error = (average_score - average_label) ** 2\n",
    "        bin_square_errors.append(len(b) * square_error)\n",
    "        \n",
    "    return math.sqrt(np.mean(bin_square_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration_error(df, tag_num):\n",
    "    ground = list(df[\"Ground Truth Indexes\"])\n",
    "    tag_confidence = list(df[f\"{tag_num}\"])\n",
    "    binary_ground = [0 if i != tag_num else 1 for i in ground]\n",
    "    return single_label_calibration_error(tag_confidence, binary_ground, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_counts = Counter()\n",
    "\n",
    "for x in df[\"Ground Truth Indexes\"]:\n",
    "    tag_counts[x] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_errors = [calibration_error(df, i) for i in range(0, num_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.237171889088204e-08"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(tag_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3651041775302343"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tag_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allen",
   "language": "python",
   "name": "allen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
